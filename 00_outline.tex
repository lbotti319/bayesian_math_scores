\documentclass[11pt]{article}
\usepackage{cite}
\usepackage[legalpaper, margin=0.75in]{geometry}

\begin{document}

\section{Introduction}

Missing data affect analysis in almost every quantitative discipline. Most common statistical methods are demonstrated on pristine data sets and the incorrect application of common statistical methods to data sets with missingness can lead to biased results. 

This paper starts with a complete data and creates missing data under different types of missingness. Bayesian data augmentation methods are used to impute the missing values and estimate a linear regression model on the imputed data. 

todo(aaron): introduce the data set

The remainder of the paper is organized as follows: Section 2...

\section{Methods}

Our objective is to estimate a linear regression model with final grade as the dependent variable and some set of predictors, but our focus is on missing data methods. Accordingly, we pick one regression specification and carry through the entire project. 

Based on theory, we chose a subset of potential predictors. First period grade, second period grade, number of previous class failures, age, sex, whether the student to take higher education, parental education, parental jobs, and number of absences were all considered. Mother's education, father's education, mother's job, and father's job were all highly correlated. Accordingly, we only include mother's education. First period grade and second period grade are also highly correlated. Accordingly, we only include second period grade because it is closer to the final grade. 

Our final specification is 

G3 ~ age + failures + sex + higher + Medu + absences + G1 + G2

\subsection{Creating Missing Data}

\textit{Unit missingness} is when data are missing for an entire observation or row. We assume the Student Performance data set contains no unit missingness. Our observations are either a simple random sample from the population or the complete population. \textit{Item missingness} is when data are missing for variables within an observation or row. Our simulated missingness will only focus on item missingness.

For our evalaution, we compare methods applied to obscured data sets with different types of missingness to methods applied to the complete data. The student grade data set has no missing values. We independently add missing values to the categorical variable for if the student wants to pursure higher education and the numeric variable for second period grade. 

We will consider missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). Defintions are available in Appendix A. 

MCAR
MAR
MNAR

It is important to note that we know the type of missingness before appying our methods, which is a best case scenario. \cite{vanBurren2018} notes that there are tests to determine MCAR vs. MAR but they aren't widely used. There are no tests to compare MAR vs. MNAR.

\subsection{1. MCAR}

Randomly drop values. 

$$p(I| \phi)$$

In this case, $\phi$ will just be a probability.

\subsection{2. MAR with ignorable missing data mechanism}

Drop values conditional on variables that will be included in the final model of interest. Furthermore, the prior distribution for $\theta$ and $\phi$ need to be independent. 

$$p(I|y_{obs}, y_{mis}, \phi) = p(I|y_{obs}, \phi)$$

Note: we can model the missingness mechanism to try to improve the model.

\subsection{3. MNAR/MAR with non-ignorable missing data mechanism}

Create missingness conditional on the values that are missing. 


\section{Results}

We implement missing data methods under the different missing data scenarios above. For each method we implement three comparisons to evaluate the performance of the methods under the different scenarios. 

First, we visually compare samples from the posterior distributions under the case with no missing data against cases with missing data. If the methods are effective, the center and spread of the posterior samples from the case with missing data should match the case without missing data. 

Second, we compare inferences with a common decision rule. For example, if the coefficient for age is different than zero for the case without missing data, is it also different than zero for cases with missing data. Ideally, each case will have a handful of inferences and the inferences should align across all cases. 

Finally, we will calculate credible interval overlap. \cite{Karr2006} introduced confidence interval overlap as a method to evaluate the utility of data after statistical disclosure control. We use the notation of \cite{Snoke2018}.

Let $U_{comp,k}$ be the upper bound of the $k^{th}$ credible interval for the case without missing data. Let $U_{miss,k}$ be the upper bound of the $k^{th}$ credible interval for the case with missing data. Let $L_{comp,k}$ be the lower bound of the $k^{th}$ credible interval for the case without missing data. Let $L_{miss,k}$ be the lower bound of the $k^{th}$ credible interval for the case with missing data. 

The credible interval overlap for the $k_th$ estimate (coefficient or mean) is 

$$J_k = \frac{1}{2}\left[\frac{min(U_{comp,k}, U_{miss,k}) - max(L_{comp,k}, L_{miss,k})}{U_{comp,k} - L_{comp,k}} + \frac{min(U_{comp,k}, U_{miss,k}) - max(L_{comp,k}, L_{miss,k})}{U_{miss,k} - L_{miss,k}}\right]$$

The interval overlap measure can then be summarized as 

$$J = \frac{1}{p}\sum_{i = 1}^pJ_k$$

where $p$ is the number of interval or coefficients. 

\section{Discussion}

\newpage
\section{Appendix A}

"Bayesian data analysis draws no distinction between missing data and parameters. Both are uncertain, and they have a joint posterior distribution, conditional on observed data." ~ BDA

A Bayesian model with missing data includes three parts:

1. A prior distribution for the parameters
2. A joint model for all of the data (missing and observed)
3. An inclusion model for the missingness process (if the missing data mechanism is non-ignorable)

\subsection{Definitions}

Let $y = (y_{obs}, y_{mis})$. Let $I$ be the inclusion indicator. Let $\theta$ be model parameters and let $\phi$ be parameters governing the missing data mechanism. Then the joint distribution of of interest is 

$$p(y, I|\theta, \phi) = p(y|\theta)p(I|y, \phi)$$

\vspace{0.25in}

\textbf{Definition:} \textit{inclusion indicator} -- A data structure with the same size and shape as as $y$ with 1 if the corresponding component is observed and 0 if the corresponding component is missing.

\vspace{0.25in}

\textbf{Definition:} \textit{inclusion model} -- The part of the statistical model that tries to model the inclusion indicator. The nature of this model is determined by the type of missingness. 

$$p(I|y_{obs}, y_{mis}, \phi)$$

\vspace{0.25in}

\textbf{Definition:} \textit{missing completely at random (MCAR)} -- If the probability of missingness is the same for all observations. Cause of missingness is unrelated to the data. A simple example is random sampling. This is also called *observed at random*.

$$p(I|y_{obs}, y_{mis}, \phi) = p(I| \phi)$$


\vspace{0.25in}

\textbf{Definition:} \textit{missing at random (MAR)} -- If the distribution of the missing data mechanism does not depend on the missing values. \textit{The distribution of the missing data mechanism can depend on fully observed values in the data and parameters for the missing data mechanism.} A simple example is a stratified random sample. 

$$p(I|y_{obs}, y_{mis}, \phi) = p(I|y_{obs}, \phi)$$

\vspace{0.25in}

\textbf{Definition:} \textit{ignorable missing data mechanism} -- If the parameters for the missing data mechanism $\phi$ and the parameters for the model $\theta$ are distinct, then the missing data mechanism is said to be ignorable. (BDA frames this as $\phi$ and $\theta$ are independent in the prior distribution). 

$$p(y_{obs},I|\theta, \phi) = p(y_{obs}|\theta)$$

In this situation

$$p(\theta|x, y_{obs}) = p(\theta|x, y_{obs}, I)$$

\vspace{0.25in}

\textbf{Definition:} \textit{missing not at random (MNAR)} -- If the missing data mechanism depends on the missing values. Neither MCAR or MAR holds. The data are missing for reasons that are unknown to us. 

\vspace{0.5in}

\textbf{Hierarchical Model}

We assume a hierarchical model to model our data. We assume that our response, $G3$ grades are normally distributed and independent:

$$\mathbf{G3} \sim MVN(\mathbf{X}\mathbf{\beta}, \sigma^2\mathbf{I})$$

To model the the explanatory variables with missing values, we consider $\texttt{Higher Yes}_i|\texttt{Age}_i$ to be a binary model. The dependence on \texttt{Age} is according to our MAR missing mechanism.

$$\texttt{Higher Yes}_i|\texttt{Age}_i \sim \texttt{Bernoulli}(\phi_i)$$

where

$$\phi_i=\frac{\exp[\alpha_0+\alpha_1\texttt{Age}_i]}{1+\exp[\alpha_0+\alpha_1\texttt{Age}_i]} \in [0,1]$$

And $\alpha_0, \alpha_1$ are given normal priors with large variance.

$$\texttt{Absences}_i|\texttt{Age}_i$$

is considered a count mode. The dependence on age is again according to our MAR missing mechanism. 

$$\texttt{G2}_i|\texttt{Age}_i \sim \texttt{Normal}(\mu_i, \eta^2)$$

where

$$\mu_i=\exp[\gamma_0+\gamma_1\texttt{Age}_i]  > 0$$

And $\gamma_0, \gamma_1$ are given normal priors with large variance.

Our parameters are then $\alpha_0, \alpha_1, \gamma_0, \gamma_1, \eta^2, \texttt{G2}_i, \texttt{Higher Yes}_i, \mathbf{\beta},$ and $\sigma^2$.

The full posterior is:

$$p(\alpha_0, \alpha_1, \gamma_0, \gamma_1, \eta^2, \texttt{G2}_i, \texttt{Higher Yes}_i, \mathbf{\beta},\sigma^2|\mathbf{G3}, \mathbf{X}_{\texttt{Rest}})$$

$$\propto L(\mathbf{G3}|\alpha_0, \alpha_1, \gamma_0, \gamma_1, \eta^2, \texttt{G2}_i, \texttt{Higher Yes}_i, \mathbf{\beta},\sigma^2, \mathbf{X}_{\texttt{Rest}})$$
$$\prod_{i=1}^{m}\pi(\texttt{G2}_i|\texttt{Age}_i)\prod_{i=1}^{n}\pi(\texttt{Higher Yes}_i|\texttt{Age}_i)\pi(\beta, \sigma^2)\pi(\alpha_0)\pi(\alpha_1)\pi(\gamma_0)\pi(\gamma_1)$$


We assume the following priors

$$\pi(\beta, \sigma^2)\propto (\sigma^2)^{-1}$$ and $\alpha_0, \alpha_1, \gamma_0, \gamma_1$ are given the flat prior 1. $\eta^2$ is given $(\eta^2)^{-1}$

And so the full posterior is proportional to,

$$(\sigma^2)^{-(n/2+1)}\exp[-\frac{1}{2\sigma^2}(\mathbf{G3}-\mathbf{X}\beta)^T(\mathbf{G3}-\mathbf{X}\beta)]$$

$$\prod_{i=1}^m \frac{1}{(\eta^2)^{1/2}} \exp \left[-\frac{1}{2\eta^2} (\texttt{G2}_i - \gamma_0 - \gamma_1 \texttt{Age}_i)^2 \right]$$

$$\prod_{i=1}^{n}(1-\frac{\exp[\alpha_0+\alpha_1\texttt{Age}_i]}{1+\exp[\alpha_0+\alpha_1\texttt{Age}_i]})^{1-\texttt{Higher Yes}_i}(\frac{\exp[\alpha_0+\alpha_1\texttt{Age}_i]}{1+\exp[\alpha_0+\alpha_1\texttt{Age}_i]})^{\texttt{Higher Yes}_i}$$

$$(\eta^2)^{-1}$$

The full conditionals are

$$p(\sigma^2|\texttt{everything else}) \sim IG(n/2,\frac{1}{2\sigma^2}(\mathbf{G3}-\mathbf{X}\beta)^T(\mathbf{G3}-\mathbf{X}\beta))$$

$$p(\beta|\texttt{everything else}) \sim MVN((\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{G3}, \sigma^2(\mathbf{X}^T\mathbf{X})^{-1})$$

For $i=1,...,m$ where $m$ is the number of missing values in $\texttt{Absences}$:

$$p(\texttt{G2}_i|\texttt{everything else}) \sim \texttt{Normal}(\gamma_0+\gamma_1\texttt{Age}_i, \eta^2)$$

For $i=1,...,n$ where $n$ is the number of missing values in $\texttt{Higher Yes}$:

$$p(\texttt{Higher Yes}_i|\texttt{everything else}) \sim \texttt{Bernoulli}(\frac{\exp[\alpha_0+\alpha_1\texttt{Age}_i]}{1+\exp[\alpha_0+\alpha_1\texttt{Age}_i]})$$

$$p(\alpha_0|\texttt{everything else})$$

$$\propto \prod_{i=1}^{n}(1-\frac{\exp[\alpha_0+\alpha_1\texttt{Age}_i]}{1+\exp[\alpha_0+\alpha_1\texttt{Age}_i]})^{1-\texttt{Higher Yes}_i}(\frac{\exp[\alpha_0+\alpha_1\texttt{Age}_i]}{1+\exp[\alpha_0+\alpha_1\texttt{Age}_i]})^{\texttt{Higher Yes}_i}$$

$$p(\alpha_1|\texttt{everything else})$$

$$\propto \prod_{i=1}^{n}(1-\frac{\exp[\alpha_0+\alpha_1\texttt{Age}_i]}{1+\exp[\alpha_0+\alpha_1\texttt{Age}_i]})^{1-\texttt{Higher Yes}_i}(\frac{\exp[\alpha_0+\alpha_1\texttt{Age}_i]}{1+\exp[\alpha_0+\alpha_1\texttt{Age}_i]})^{\texttt{Higher Yes}_i}$$

$$p(\gamma_0|\texttt{everything else}) \propto \prod_{i=1}^m  \exp \left[-\frac{1}{2\eta^2} (\texttt{G2}_i - \gamma_0 - \gamma_1 \texttt{Age}_i)^2 \right]$$

$$p(\gamma_0 | ...) \propto \exp \left[ -\frac{1}{2\eta^2} \sum (-\gamma_0 + \texttt{G2}_i - \gamma_1 \texttt{Age}_i)^2 \right]$$

$$p(\gamma_0 | ...) \propto \exp \left[ -\frac{1}{2\eta^2} \sum \gamma_0^2 - 2\gamma_0(\texttt{G2}_i - \gamma_1 \texttt{Age}_i) 
+  (\texttt{G2}_i - \gamma_1 \texttt{Age}_i)^2 \right]$$

$$p(\gamma_0 | ...) \propto \exp \left[ -\frac{1}{2\eta^2} (m\gamma_0^2 - 2\gamma_0\sum(\texttt{G2}_i - \gamma_1 \texttt{Age}_i)) \right]$$

$$p(\gamma_0 | ...) \propto \exp \left[ -\frac{m}{2\eta^2} (\gamma_0^2 - \frac{2}{m}\gamma_0\sum(\texttt{G2}_i - \gamma_1 \texttt{Age}_i)) \right]$$

$$\gamma_0 | ... \sim N\left( \frac{\sum(\texttt{G2}_i - \gamma_1 \texttt{Age}_i)}{m}, \frac{\eta^2}{m} \right)$$

$$p(\gamma_1|\texttt{everything else}) \propto \prod_{i=1}^m  \exp \left[-\frac{1}{2\eta^2} (\texttt{G2}_i - \gamma_0 - \gamma_1 \texttt{Age}_i)^2 \right]
$$

$$p(\gamma_1 | ...) \propto \exp \left[ -\frac{1}{2\eta^2} \sum (- \gamma_1 \texttt{Age}_i + \texttt{G2}_i -\gamma_0 )^2 \right]$$

$$p(\gamma_1 | ...) \propto \exp \left[ -\frac{1}{2\eta^2} \sum (\gamma_1 \texttt{Age}_i)^2 
-2 (\gamma_1 \texttt{Age}_i)(\texttt{G2}_i -\gamma_0 )
+ (\texttt{G2}_i -\gamma_0 )^2 \right]$$

$$p(\gamma_1 | ...) \propto \exp \left[ -\frac{1}{2\eta^2} 
(\gamma_1^2 \sum \texttt{Age}_i^2 
-2 \gamma_1 \sum \texttt{Age}_i(\texttt{G2}_i -\gamma_0 ))\right]$$

$$p(\gamma_1 | ...) \propto \exp \left[ -\frac{\sum\texttt{Age}_i^2}{2\eta^2} 
(\gamma_1^2
-2 \gamma_1 \frac{\sum \texttt{Age}_i(\texttt{G2}_i -\gamma_0 )}{\sum\texttt{Age}_i^2})\right]$$

$$\gamma_1 | ... \sim N\left( 
\frac{\sum \texttt{Age}_i(\texttt{G2}_i -\gamma_0 )}{\sum\texttt{Age}_i^2},
\frac{\eta^2}{\sum\texttt{Age}_i^2} \right)$$

$$p(\eta^2 | \texttt{everything else}) \propto IG \left(
\frac{m}{2},
\frac{1}{2}\sum_{i=1}^m (\texttt{G2}_i - \gamma_0 - \gamma_1\texttt{Age}_i)^2
\right)$$


\newpage
\section{Appendix B}

\subsubsection{Setup}

Let $y = (y_{mis}, y_{obs})$ and $\mathbf{X} = (\mathbf{X}_{mis}, \mathbf{X}_{obs})$

We assume the distribution of $\vec{Y}$ is

$$Y \sim MVN(\mathbf{X}\vec{\beta}, \sigma^2\mathbf{I})$$

We assume a non-informative prior with $\vec{\beta}$ with a flat prior and $\sigma^2$ with Jeffreys' prior

$$p(\vec{\beta}, \sigma^2) \propto (\sigma^2)^{-1}$$

\vspace{0.25in}

We include two imputation models. First, for missing values of G2, we assume a normal distribution such that

$$X_{G2} \sim MVN(\mathbf{X}_{Age}\vec{\gamma}, \eta^2\mathbf{I})$$

We assume a non-informative prior with $\vec{\gamma}$ with a flat prior and $\eta^2$ with Jeffreys' prior

$$p(\vec{\gamma}, \eta^2) \propto (\eta^2)^{-1}$$

\vspace{0.25in}

Second, for missing values of Higher, we assume a Bernoulli distribution and we use Normal approximation for logistic regression. So the likelihood $\mathcal{L}(y_i|x_i'\vec\alpha, \Phi)$ can be approximated by the normal likelihood $N(z_i, \theta^2_i)$.

\subsubsection{Sampling Procedure}

The sampling procedure is similar to an EM algorithm. There are imputation steps and then conditional posterior steps. Use the sample mean for G2, "Yes" for Higher (sample mode), and OLS estimates from the complete data for everything else for all starting values. The Gibbs sample has the following steps:

\vspace{0.25in}

\textbf{Imputation step 1.} $X_{mis, G2}|\vec\gamma, \eta, X_{mis, Age}$

$$X_{mis, G2}^{(b)}|\vec\gamma^{(b - 1)}, \eta^{2(b - 1)}, X_{mis, Age} \sim MVN(\mathbf{X}\vec{\gamma}^{(b - 1)}, \eta^{2(b - 1)}\mathbf{I})$$

Where $\mathbf{X}$ is a design matrix with $X_{Age}$

This is derived on slides 169-173 in notes set 1

\vspace{0.25in}

\textbf{Imputation step 2.} $X_{mis, Higher}|\alpha_0, \alpha_1, X_{mis, Age}$

$$X_{mis, Higher}^{(b)}|\vec\alpha^{(b - 1)}, X_{mis, Age} \sim Bernoulli\left(\frac{exp(\alpha_0 + \alpha_1X_{age})}{1 + exp(\alpha_0 + \alpha_1X_{age})}\right)$$

\textbf{Conditional posterior 1.} $\vec\beta | \sigma^2, y, \mathbf{X}$

$$\vec\beta^{(b)} | \sigma^{2(b - 1)}, y, \mathbf{X} \sim MVN((\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\vec{Y}, \sigma^{2(b - 1)}(\mathbf{X}^T\mathbf{X}^{-1}))$$

Where $\mathbf{X}$ is a design matrix all predictors.

This is derived on slides 169-173 in notes set 1

\vspace{0.25in}

\textbf{Conditional posterior 2.} $\sigma^2 | \vec\beta, y, \mathbf{X}$

$$\sigma^{2(b)} | \vec\beta^{(b - 1)}, y, \mathbf{X} \sim IG\left(\frac{n - k}{2}, \frac{1}{2}(\vec{Y} - \mathbf{X}\vec{\hat\beta^{(b - 1)}})^T(\vec{Y} - \mathbf{X}\vec{\hat\beta^{(b - 1)}})\right)$$

Where $\mathbf{X}$ is a design matrix all predictors.

This is derived on slides 169-173 in notes set 1

\vspace{0.25in}

\textbf{Conditional posterior 3.} $\vec{\gamma}|\eta^2, X_{obs, GS}, X_{obs, Age}, X_{mis, GS}, X_{mis, Age}$

$$\vec{\gamma}^{(b)}|\eta^{2(b - 1)}, X_{obs, GS}, X_{obs, Age}, X_{mis, GS}, X_{mis, Age} \sim MVN((\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\vec{X}_{G2}, \eta^{2(b - 1)}(\mathbf{X}^T\mathbf{X}^{-1}))$$

Where $\mathbf{X}$ is a design matrix with $X_{Age}$

This is derived on slides 169-173 in notes set 1

\vspace{0.25in}

\textbf{Conditional posterior 4.} $\eta^2|\vec{\gamma}, X_{obs, GS}, X_{obs, Age}, X_{mis, GS}, X_{mis, Age}$

$$\eta^{2(b)}|\vec{\gamma}^{(b - 1)}, X_{obs, GS}, X_{obs, Age}, X_{mis, GS}, X_{mis, Age} \sim IG\left(\frac{n - k}{2}, \frac{1}{2}(\vec{X}_{G2} - \mathbf{X}\vec{\hat\gamma^{(b - 1)}})^T(\vec{X}_{G2} - \mathbf{X}\vec{\hat\gamma^{(b - 1)}})\right)$$

Where $\mathbf{X}$ is a design matrix with $X_{Age}$

This is derived on slides 169-173 in notes set 1

\vspace{0.25in}

\textbf{Conditional posterior 5.} $\vec\alpha| \theta^2, X_{obs, Higher}, X_{obs, Age}, X_{mis, Higher}, X_{mis, Age}$

$$\vec\alpha^{(b)}|  X_{obs, Higher}, X_{obs, Age}, X_{mis, Higher}, X_{mis, Age} \sim MVN(\hat{\vec\alpha}, V_{\alpha})$$

Where $\hat{\vec\alpha}$ is the posterior mode found through Newton-Raphson and $V_{\alpha}$ is the last working variance from iterative solution to the posterior mode

This is derived on slides 206-208 in notes set 1.

\newpage

\bibliography{references}
\bibliographystyle{apalike}

\end{document}




